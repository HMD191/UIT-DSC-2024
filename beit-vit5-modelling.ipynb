{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9571267,"sourceType":"datasetVersion","datasetId":5834049},{"sourceId":9660707,"sourceType":"datasetVersion","datasetId":5902254},{"sourceId":9728617,"sourceType":"datasetVersion","datasetId":5953272},{"sourceId":9774475,"sourceType":"datasetVersion","datasetId":5987472}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Import Libraries and Load Dataset","metadata":{}},{"cell_type":"code","source":"%%capture\n#!git clone https://github.com/huggingface/transformers.git\n!pip install datasets evaluate transformers[sentencepiece]\n!pip install pyvi\n!pip install gdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:32:01.723226Z","iopub.execute_input":"2024-11-01T07:32:01.723860Z","iopub.status.idle":"2024-11-01T07:32:38.848356Z","shell.execute_reply.started":"2024-11-01T07:32:01.723823Z","shell.execute_reply":"2024-11-01T07:32:38.847174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%%capture\n\nimport gdown\n\n!gdown 1jWDnP1xF01_pmeJYsRHxSVBkLZ0I9LAt #utils T5 for VQA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:32:38.850754Z","iopub.execute_input":"2024-11-01T07:32:38.851249Z","iopub.status.idle":"2024-11-01T07:32:45.166247Z","shell.execute_reply.started":"2024-11-01T07:32:38.851212Z","shell.execute_reply":"2024-11-01T07:32:45.165100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\nimport requests\n\nimport os\nimport shutil\nimport torch\n\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimport tqdm\nimport torch\nfrom IPython.display import FileLink","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:32:45.167676Z","iopub.execute_input":"2024-11-01T07:32:45.168349Z","iopub.status.idle":"2024-11-01T07:32:49.615541Z","shell.execute_reply.started":"2024-11-01T07:32:45.168321Z","shell.execute_reply":"2024-11-01T07:32:49.614582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\ntrain_json = json.load(open('/kaggle/input/dsc24-vimmsd/vimmsd-train.json', encoding='utf-8'))\ndev_json = json.load(open('/kaggle/input/dsc24-vimmsd/vimmsd-public-test.json', encoding='utf-8'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:32:49.618078Z","iopub.execute_input":"2024-11-01T07:32:49.618394Z","iopub.status.idle":"2024-11-01T07:32:49.796698Z","shell.execute_reply.started":"2024-11-01T07:32:49.618370Z","shell.execute_reply":"2024-11-01T07:32:49.795932Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image feature extraction","metadata":{}},{"cell_type":"code","source":"# images = []\n\n# for key, item in train_json.items():\n#     images.append(item['image'])\n\n# for key, item in dev_json.items():\n#     images.append(item['image'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:32:49.797798Z","iopub.execute_input":"2024-11-01T07:32:49.798149Z","iopub.status.idle":"2024-11-01T07:32:49.802794Z","shell.execute_reply.started":"2024-11-01T07:32:49.798102Z","shell.execute_reply":"2024-11-01T07:32:49.801803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# source_all = ['/kaggle/input/dsc24-vimmsd/dev-images/dev-images',\n#               '/kaggle/input/dsc24-vimmsd/train-images/train-images']\n\n# destination = './images'\n# #os.mkdir(destination)\n\n# for source in source_all:\n#     allfiles = os.listdir(source)\n#     for f in allfiles:\n#         src_path = os.path.join(source, f)\n#         dst_path = os.path.join(destination, f)\n#         shutil.copy(src_path, dst_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:32:49.803930Z","iopub.execute_input":"2024-11-01T07:32:49.804232Z","iopub.status.idle":"2024-11-01T07:32:49.813187Z","shell.execute_reply.started":"2024-11-01T07:32:49.804203Z","shell.execute_reply":"2024-11-01T07:32:49.812292Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## BEiT","metadata":{}},{"cell_type":"code","source":"# model = timm.create_model(\n#     'beitv2_base_patch16_224.in1k_ft_in22k_in1k',\n#     pretrained=True,\n#     num_classes=0,  # remove classifier nn.Linear\n# ).to('cuda')\n\n# model = model.eval()\n# data_config = timm.data.resolve_model_data_config(model)\n# transforms = timm.data.create_transform(**data_config, is_training=False)\n\n# img_w = {}\n\n# def batch(iterable, n=1):\n#     l = len(iterable)\n#     for ndx in range(0, l, n):\n#         yield iterable[ndx:min(ndx + n, l)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:32:49.814285Z","iopub.execute_input":"2024-11-01T07:32:49.814545Z","iopub.status.idle":"2024-11-01T07:32:49.822873Z","shell.execute_reply.started":"2024-11-01T07:32:49.814521Z","shell.execute_reply":"2024-11-01T07:32:49.822027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# p = '/kaggle/working/images/'\n\n# img_w = {}\n\n# # get features for images, we will do this 3 images at a time to reduce time\n\n# for x in tqdm.notebook.tqdm(batch(images, 3),total=int(len(images)/3)):\n#     img = [Image.open(p + v).convert('RGB') for v in x]\n#     print(x)\n\n#     with torch.no_grad():\n#         img  = torch.stack([transforms(i) for i in img]).to('cuda')\n#         output = model.forward_features(img)[:,1:,:]\n\n#     tmp_img_w = {k:v for k,v in zip(x,output)}\n\n#     img_w.update(tmp_img_w)\n\n#     del output\n#     del tmp_img_w\n#     del img\n#     torch.cuda.empty_cache()\n\n# torch.save(img_w, '/kaggle/working/beitv2-b-p.pt') # export for later used\n# FileLink('/kaggle/working/beitv2-b-p.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:32:49.824068Z","iopub.execute_input":"2024-11-01T07:32:49.824411Z","iopub.status.idle":"2024-11-01T07:32:49.832878Z","shell.execute_reply.started":"2024-11-01T07:32:49.824382Z","shell.execute_reply":"2024-11-01T07:32:49.832176Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load extracted image features","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.nn.functional import normalize\n\nimg_w = torch.load('/kaggle/input/dsc24-vit/vit-b.pt')\n# img_w = torch.load('/kaggle/input/lovecat-beitv2-b-p/beitv2-b-p.pt') # already-saved features\nlen(img_w)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:32:49.833966Z","iopub.execute_input":"2024-11-01T07:32:49.834274Z","iopub.status.idle":"2024-11-01T07:33:46.307234Z","shell.execute_reply.started":"2024-11-01T07:32:49.834245Z","shell.execute_reply":"2024-11-01T07:33:46.306358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_w['ac7931bb887ad853b41675f07595bf04469970d1b099ffc8806a4ceaac7d7940.jpg'].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:33:46.311050Z","iopub.execute_input":"2024-11-01T07:33:46.311363Z","iopub.status.idle":"2024-11-01T07:33:46.317110Z","shell.execute_reply.started":"2024-11-01T07:33:46.311337Z","shell.execute_reply":"2024-11-01T07:33:46.316271Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/HMD191/UIT-DSC-2024.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T12:04:05.094140Z","iopub.execute_input":"2024-11-03T12:04:05.095046Z","iopub.status.idle":"2024-11-03T12:04:06.804656Z","shell.execute_reply.started":"2024-11-03T12:04:05.095011Z","shell.execute_reply":"2024-11-03T12:04:06.803784Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'UIT-DSC-2024'...\nremote: Enumerating objects: 48, done.\u001b[K\nremote: Counting objects: 100% (48/48), done.\u001b[K\nremote: Compressing objects: 100% (44/44), done.\u001b[K\nremote: Total 48 (delta 14), reused 21 (delta 1), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (48/48), 868.77 KiB | 15.79 MiB/s, done.\nResolving deltas: 100% (14/14), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd UIT-DSC-2024/VisualBERT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T12:04:07.111546Z","iopub.execute_input":"2024-11-03T12:04:07.111903Z","iopub.status.idle":"2024-11-03T12:04:07.118831Z","shell.execute_reply.started":"2024-11-03T12:04:07.111873Z","shell.execute_reply":"2024-11-03T12:04:07.117849Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/UIT-DSC-2024/VisualBERT\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!python -m pip install pyyaml==5.1\nimport sys, os, distutils.core\n# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n!git clone 'https://github.com/facebookresearch/detectron2'\ndist = distutils.core.run_setup(\"./detectron2/setup.py\")\n!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\nsys.path.insert(0, os.path.abspath('./detectron2'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T11:59:10.673540Z","iopub.execute_input":"2024-11-03T11:59:10.674334Z","iopub.status.idle":"2024-11-03T11:59:30.171461Z","shell.execute_reply.started":"2024-11-03T11:59:10.674289Z","shell.execute_reply":"2024-11-03T11:59:30.170308Z"}},"outputs":[{"name":"stdout","text":"Collecting pyyaml==5.1\n  Downloading PyYAML-5.1.tar.gz (274 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: pyyaml\n  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.1-cp310-cp310-linux_x86_64.whl size=44090 sha256=a54af7cf48c57c5f2aff769d39058df55729e2e2fcac49bd16fffd3e06d38e9f\n  Stored in directory: /root/.cache/pip/wheels/70/83/31/975b737609aba39a4099d471d5684141c1fdc3404f97e7f68a\nSuccessfully built pyyaml\nInstalling collected packages: pyyaml\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0\n    Uninstalling PyYAML-6.0:\n      Successfully uninstalled PyYAML-6.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask 2023.9.0 requires pyyaml>=5.3.1, but you have pyyaml 5.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndistributed 2023.7.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1 which is incompatible.\nflax 0.7.2 requires PyYAML>=5.4.1, but you have pyyaml 5.1 which is incompatible.\njupyter-events 0.6.3 requires pyyaml>=5.3, but you have pyyaml 5.1 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.0.1 requires PyYAML<7,>=5.3, but you have pyyaml 5.1 which is incompatible.\nkubernetes 26.1.0 requires pyyaml>=5.4.1, but you have pyyaml 5.1 which is incompatible.\npytorch-lightning 2.0.8 requires PyYAML>=5.4, but you have pyyaml 5.1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pyyaml-5.1\nCloning into 'detectron2'...\nremote: Enumerating objects: 15792, done.\u001b[K\nremote: Counting objects: 100% (49/49), done.\u001b[K\nremote: Compressing objects: 100% (41/41), done.\u001b[K\nremote: Total 15792 (delta 15), reused 27 (delta 8), pack-reused 15743 (from 1)\u001b[K\nReceiving objects: 100% (15792/15792), 6.36 MiB | 16.13 MiB/s, done.\nResolving deltas: 100% (11509/11509), done.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\u001b[39;00m\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit clone \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/facebookresearch/detectron2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mdistutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./detectron2/setup.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython -m pip install \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m.join([f\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;132;01m{x}\u001b[39;00m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for x in dist.install_requires])}\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./detectron2\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/core.py:267\u001b[0m, in \u001b[0;36mrun_setup\u001b[0;34m(script_name, script_args, stop_after)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tokenize\u001b[38;5;241m.\u001b[39mopen(script_name) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    266\u001b[0m         code \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m         \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m save_argv\n","File \u001b[0;32m<string>:153\u001b[0m\n","File \u001b[0;32m<string>:18\u001b[0m, in \u001b[0;36mget_version\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/posixpath.py:384\u001b[0m, in \u001b[0;36mabspath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    382\u001b[0m         cwd \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwdb()\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m         cwd \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     path \u001b[38;5;241m=\u001b[39m join(cwd, path)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normpath(path)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"%%writefile visualbert.py\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.nn.functional import softmax\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\n\nfrom transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom transformers import VisualBertModel, VisualBertConfig\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, multilabel_confusion_matrix, f1_score\n\nfrom visual_embeds import *\nfrom utils import *\n\nRANDOM_SEED = 42\nMAX_LEN = 64\nN_CLASSES = 2\nN_EPOCHS = 10\nBATCH_SIZE = 32\n\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\npl.seed_everything(RANDOM_SEED)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass MemesDataset(Dataset):\n    '''Wrap the tokenization process in a PyTorch Dataset, along with converting the labels to tensors'''\n    \n    def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_len: int, visual_embeds):\n        self.tokenizer = tokenizer\n        self.data = data\n        self.max_len = max_len\n        self.visual_embeds = visual_embeds\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n\n        data_row = self.data.iloc[index]\n        text = data_row.caption\n        labels = data_row.misogynous\n\n        tokens = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        input_ids = torch.tensor(tokens[\"input_ids\"]).flatten()\n        attention_mask = torch.tensor(tokens[\"attention_mask\"]).flatten()\n\n        visual_embedding = self.visual_embeds[index].to('cpu')\n        visual_attention_mask = torch.ones(visual_embedding.shape[:-1], dtype=torch.float)\n        visual_token_type_ids = torch.ones(self.visual_embeds.shape[:-1], dtype=torch.long)\n\n        return dict(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            visual_embedding=visual_embedding,\n            visual_attention_mask=visual_attention_mask,\n            visual_token_type_ids=visual_token_type_ids,\n            labels=torch.tensor(labels).float()\n        )\n\n\nclass MemesDataModule(pl.LightningDataModule):\n    '''\n    1- Split the dataset into training and validation dataset\n    2- Create Dataloaders from Datasets (Divide data into batches)\n    '''\n\n    def __init__(self, df, tokenizer, visual_embeds, batch_size=32, max_len=64):\n        super().__init__()\n        self.batch_size = batch_size\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.visual_embeds = visual_embeds\n  \n    def setup(self, stage=None):\n        self.dataset = MemesDataset(self.df, self.tokenizer, self.max_len, self.visual_embeds)\n        self.train_dataset, self.val_dataset = train_test_split(self.dataset, test_size=0.1) # Split the dataset into training and validation datasets\n  \n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size = self.batch_size,\n            shuffle=True,\n            num_workers=3\n        )\n  \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size = self.batch_size,\n            num_workers=3\n        )\n  \n    def test_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size = self.batch_size,\n            num_workers=3\n        )\n\n\nclass MemesClassifier(pl.LightningModule):\n  '''Wrap the training of VisualBERT model to classify memes'''\n\n  def __init__(self, n_classes, n_training_steps=None, n_warmup_steps=None):\n    super().__init__()\n    self.configuration = VisualBertConfig.from_pretrained('uclanlp/visualbert-vqa-coco-pre', visual_embedding_dim=1024)\n    self.model = VisualBertModel(self.configuration)\n    self.n_training_steps = n_training_steps\n    self.n_warmup_steps = n_warmup_steps\n    self.criterion = nn.CrossEntropyLoss()\n    self.dropout = nn.Dropout(0.2)\n    self.classifier = nn.Linear(self.model.config.hidden_size, n_classes)\n  \n  \n  def forward(self, input_ids, attention_mask, visual_embeds, visual_attention_mask, visual_token_type_ids, labels=None):\n    output = self.model(input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        visual_embeds=visual_embeds,\n                        visual_attention_mask=visual_attention_mask,\n                        visual_token_type_ids=visual_token_type_ids\n    )\n    \n    output = self.dropout(output.pooler_output)\n    output = self.classifier(output)\n\n    loss = 0\n    if labels is not None:\n      loss = self.criterion(output, labels)\n\n    return loss, output\n  \n  def training_step(self, batch, batch_idx):\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    visual_embeds = batch['visual_embedding'].to(device)\n    visual_attention_mask = batch['visual_attention_mask'].to(device)\n    visual_token_type_ids = batch['visual_token_type_ids'].to(device)\n\n    labels = batch['labels'].type(torch.LongTensor).to(device)\n    \n    loss, outputs = self(input_ids, attention_mask, visual_embeds, visual_attention_mask, visual_token_type_ids, labels)\n    self.log('train_loss', loss, prog_bar=True, logger=True)\n\n    return {\"loss\":loss, 'predictions':outputs, 'labels':labels}\n\n  def validation_step(self, batch, batch_idx):\n    input_ids = batch['input_ids']\n    attention_mask = batch['attention_mask']\n    visual_embeds = batch['visual_embedding']\n    visual_attention_mask = batch['visual_attention_mask']\n    visual_token_type_ids = batch['visual_token_type_ids'].to(device)\n    labels = batch['labels'].type(torch.LongTensor).to(device)\n    \n    loss, outputs = self(input_ids, attention_mask, visual_embeds, visual_attention_mask, visual_token_type_ids, labels)\n    self.log('val_loss', loss, prog_bar=True, logger=True)\n\n    return loss\n  \n  def configure_optimizers(self):\n    optimizer = AdamW(self.parameters(), lr=5e-5)\n\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=self.n_warmup_steps,\n        num_training_steps=self.n_training_steps\n    )\n\n    return dict(\n        optimizer=optimizer,\n        lr_scheduler=dict(\n            scheduler=scheduler,\n            interval='step'\n        )\n    )\n\n\ndef train_model(model, df, tokenizer, visual_embeds):\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=\"checkpoints\",\n        filename=\"best-checkpoint\",\n        save_top_k=1,\n        verbose=True,\n        monitor=\"val_loss\",\n        mode=\"min\",\n    )\n    \n    logger = TensorBoardLogger(\"lightning_logs\", name=\"memes-text\")\n    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n\n    data_module = MemesDataModule(\n        df=df,\n        tokenizer=tokenizer,\n        visual_embeds=visual_embeds,\n        batch_size=BATCH_SIZE,\n        max_len=MAX_LEN\n    )\n\n    trainer = pl.Trainer(\n        logger=logger,\n        callbacks=[early_stopping_callback, checkpoint_callback],\n        max_epochs=N_EPOCHS,\n        gpus=1,\n        progress_bar_refresh_rate=10\n    )\n\n    trainer.fit(model, data_module)\n\n\ndef evaluate_model(test_dataset, checkpoint):\n    trained_model = MemesClassifier.load_from_checkpoint(\n        checkpoint,\n        n_classes=2\n    ).to(device)\n\n    trained_model.eval()\n    trained_model.freeze()\n\n    predictions = []\n    labels = []\n    for item in tqdm(test_dataset):\n        _, prediction = trained_model(\n            item[\"input_ids\"].unsqueeze(dim=0).to(device),\n            item[\"attention_mask\"].unsqueeze(dim=0).to(device),\n            item[\"visual_embedding\"].unsqueeze(dim=0).to(device),\n            item['visual_attention_mask'].unsqueeze(dim=0).to(device),\n            item['visual_token_type_ids'].unsqueeze(dim=0).to(device)\n        )\n        predictions.append(prediction.flatten())\n        labels.append(item[\"labels\"].int())\n    \n    predictions = torch.stack(predictions).detach().cpu()\n    labels = torch.stack(labels).detach().cpu()\n\n    _, preds = torch.max(torch.tensor(predictions), dim=1)\n    \n    f1_macro = f1_score(labels, preds , average=\"macro\")\n    f1_micro = f1_score(labels, preds , average=\"micro\")\n    accuracy = accuracy(predictions, labels)\n\n    return f1_macro, f1_micro, accuracy\n    \n\ndef main(): \n    df = pd.read_csv('./Data/your_training.csv')\n    df.text = np.array([preprocess_text(text) for text in df.text])\n\n    visual_embeds = generate_visual_embeds(\n        images_file_names=df.file_name,\n        images_path='./Data/Images', # You can put your images in this folder or change the path\n        cfg_path='COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml'\n    )\n    \n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    steps_per_epoch = 8000 // BATCH_SIZE\n    total_training_steps = steps_per_epoch * N_EPOCHS\n    warmup_steps = total_training_steps // 10\n\n    model = MemesClassifier(\n        n_classes=N_CLASSES,\n        n_training_steps=total_training_steps,\n        n_warmup_steps=warmup_steps,\n    ).to(device)\n\n    train_model(\n        model=model, \n        df=df,\n        tokenizer=tokenizer,\n        visual_embeds=visual_embeds\n    )\n\n    test_df = pd.read_csv('./Data/your_test.csv')\n    test_dataset = MemesDataset(\n        test_df, \n        tokenizer, \n        MAX_LEN\n    )\n\n    f1_macro, f1_micro, accuracy = evaluate_model(\n        test_dataset=test_dataset,\n        checkpoint='./checkpoints/best-checkpoint.ckpt'\n    )\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T12:08:17.524528Z","iopub.execute_input":"2024-11-03T12:08:17.524899Z","iopub.status.idle":"2024-11-03T12:08:17.538161Z","shell.execute_reply.started":"2024-11-03T12:08:17.524866Z","shell.execute_reply":"2024-11-03T12:08:17.537167Z"}},"outputs":[{"name":"stdout","text":"Overwriting visualbert.py\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import pickle\nimport re\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T12:07:08.281088Z","iopub.execute_input":"2024-11-03T12:07:08.281452Z","iopub.status.idle":"2024-11-03T12:07:08.460048Z","shell.execute_reply.started":"2024-11-03T12:07:08.281422Z","shell.execute_reply":"2024-11-03T12:07:08.459041Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"%%writefile requirements.txt\ntorch\ntorchMetrics\ntransformers\npandas\npytorch-lightning\nmatplotlib\nseaborn\nnumpy\nnltk\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T12:07:09.243163Z","iopub.execute_input":"2024-11-03T12:07:09.243526Z","iopub.status.idle":"2024-11-03T12:07:09.249841Z","shell.execute_reply.started":"2024-11-03T12:07:09.243498Z","shell.execute_reply":"2024-11-03T12:07:09.248753Z"}},"outputs":[{"name":"stdout","text":"Overwriting requirements.txt\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T12:07:09.887911Z","iopub.execute_input":"2024-11-03T12:07:09.888870Z","iopub.status.idle":"2024-11-03T12:07:22.206419Z","shell.execute_reply.started":"2024-11-03T12:07:09.888825Z","shell.execute_reply":"2024-11-03T12:07:22.205445Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: torchMetrics in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.1.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.33.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.0.2)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (2.0.8)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (3.7.2)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.12.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.23.5)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (3.2.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchMetrics->-r requirements.txt (line 2)) (0.9.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.16.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (4.66.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 4)) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 4)) (2023.3)\nRequirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning->-r requirements.txt (line 5)) (2023.9.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (9.5.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (3.0.9)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 9)) (1.16.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 5)) (3.8.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 5)) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 5)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 5)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 5)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 5)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->-r requirements.txt (line 5)) (1.3.1)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from visualbert import MemesDataset, MemesClassifer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T11:57:53.601508Z","iopub.execute_input":"2024-11-03T11:57:53.602241Z","iopub.status.idle":"2024-11-03T11:57:55.481014Z","shell.execute_reply.started":"2024-11-03T11:57:53.602206Z","shell.execute_reply":"2024-11-03T11:57:55.479650Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvisualbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MemesDataset\n","File \u001b[0;32m/kaggle/working/UIT-DSC-2024/VisualBERT/visualbert.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, multilabel_confusion_matrix, f1_score\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvisual_embeds\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m RANDOM_SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n","File \u001b[0;32m/kaggle/working/UIT-DSC-2024/VisualBERT/visual_embeds.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_model\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetectionCheckpointer\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstructures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageList\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detectron2'"],"ename":"ModuleNotFoundError","evalue":"No module named 'detectron2'","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"# df = pd.read_csv('./Data/your_training.csv') --> train_df\n\nvisual_embeds = generate_visual_embeds(\n    images_file_names=df.image,\n    images_path='/kaggle/input/dsc24-vimmsd/train-images/train-images', # You can put your images in this folder or change the path\n    cfg_path='COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml'\n)\n\ntokenizer = BertTokenizer.from_pretrained('uitnlp/visobert')\n\nsteps_per_epoch = 8000 // 8\ntotal_training_steps = steps_per_epoch * 2\nwarmup_steps = total_training_steps // 10\n\nmodel = MemesClassifier(\n    n_classes=4,\n    n_training_steps=total_training_steps,\n    n_warmup_steps=warmup_steps,\n).to(device)\n\ntrain_model(\n    model=model, \n    df=df,\n    tokenizer=tokenizer,\n    visual_embeds=visual_embeds\n)\n\nval_dataset = MemesDataset(\n    val_df, \n    tokenizer, \n    MAX_LEN\n)\n\nf1_macro, f1_micro, accuracy = evaluate_model(\n    test_dataset=val_dataset,\n    checkpoint='./checkpoints/best-checkpoint.ckpt'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load checkpoint and predict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare datasets","metadata":{}},{"cell_type":"code","source":"emoji_file_path = '/kaggle/input/datasets-preprocesing/emoji_to_vietnamese.json'\nstopword_path = '/kaggle/input/datasets-preprocesing/vietnamese-stopwords.txt'\n\ndef load_resources(stopword_path, emoji_file_path):\n    # Đọc stopword từ file txt\n    with open(stopword_path, 'r', encoding='utf-8') as f:\n        stopwords = set(f.read().splitlines())\n\n    # Đọc emoji từ file JSON\n    with open(emoji_file_path, 'r', encoding='utf-8') as emoji_file:\n        emoji_dict = json.load(emoji_file)\n\n    return stopwords, emoji_dict\n\nstopwords, emoji_dict = load_resources(stopword_path, emoji_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:08.539264Z","iopub.execute_input":"2024-11-01T07:34:08.539864Z","iopub.status.idle":"2024-11-01T07:34:08.568636Z","shell.execute_reply.started":"2024-11-01T07:34:08.539834Z","shell.execute_reply":"2024-11-01T07:34:08.567742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.DataFrame(train_json).T\ntest_df = pd.DataFrame(dev_json).T # public test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:08.569775Z","iopub.execute_input":"2024-11-01T07:34:08.572009Z","iopub.status.idle":"2024-11-01T07:34:10.505546Z","shell.execute_reply.started":"2024-11-01T07:34:08.571973Z","shell.execute_reply":"2024-11-01T07:34:10.504758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['image_id'] = train_df['image'].astype(str)\ntest_df['image_id'] = test_df['image'].astype(str)\n\ntrain_df['caption'] = train_df['caption'].astype(str)\ntest_df['caption'] = test_df['caption'].astype(str)\n\ntrain_df['label'] = train_df['label'].astype(str)\ntest_df['label'] = test_df['label'].astype(str)\n\ntrain_df.drop(columns=['image'], inplace=True)\ntest_df.drop(columns=['image'], inplace=True)\n\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:10.506664Z","iopub.execute_input":"2024-11-01T07:34:10.506953Z","iopub.status.idle":"2024-11-01T07:34:10.543092Z","shell.execute_reply.started":"2024-11-01T07:34:10.506928Z","shell.execute_reply":"2024-11-01T07:34:10.542168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Features enrichment","metadata":{}},{"cell_type":"code","source":"!gdown 1q7_-PEQQ6IR3Ortz45vEiSiOnweRJ40H # cap train\n!gdown 1wldmw8IJgX-nK2_yfo8fLx55KfWGIZhp # cap dev ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:10.544269Z","iopub.execute_input":"2024-11-01T07:34:10.544546Z","iopub.status.idle":"2024-11-01T07:34:26.380532Z","shell.execute_reply.started":"2024-11-01T07:34:10.544519Z","shell.execute_reply":"2024-11-01T07:34:26.379411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown 1AnM0RUMfyGYWaiUgafufEKMjB8zo5dUt # object reg train\n!gdown 1sk2vJutRJLCUwQ6ZKJQeYFKpvdkjUiBs # object reg dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:26.382165Z","iopub.execute_input":"2024-11-01T07:34:26.382546Z","iopub.status.idle":"2024-11-01T07:34:38.092101Z","shell.execute_reply.started":"2024-11-01T07:34:26.382510Z","shell.execute_reply":"2024-11-01T07:34:38.090981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown 1nh3y-lXq2CEc_rwzeTqGIU69VZA4eVEn # OCR dev\n!gdown 1YSn-dWwprc0nhOgRUIj5aFPaW9lxKWZT","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Open JSON files with utf-8 encoding to handle non-ASCII characters\nwith open('/kaggle/working/vi_train_captions.json', encoding='utf-8') as f:\n    cap_train = json.load(f)\n\nwith open('/kaggle/working/vi_dev_captions.json', encoding='utf-8') as f:\n    cap_test = json.load(f)\n\nwith open('/kaggle/working/objects-recognition-train.json', encoding='utf-8') as f:\n    obj_train = json.load(f)\n\nwith open('/kaggle/working/objects-recognition-dev.json', encoding='utf-8') as f:\n    obj_test = json.load(f)\n\nwith open('/kaggle/working/ocr-results-dev.json', encoding='utf-8') as f:\n    ocr_test = json.load(f)\n\nwith open('/kaggle/working/ocr-results-train.json', encoding='utf-8') as f:\n    ocr_train = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:43.613026Z","iopub.execute_input":"2024-11-01T07:34:43.613358Z","iopub.status.idle":"2024-11-01T07:34:43.737421Z","shell.execute_reply.started":"2024-11-01T07:34:43.613331Z","shell.execute_reply":"2024-11-01T07:34:43.736587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def json_to_df(json):\n    df = pd.DataFrame(json)\n    df['image_id'] = df['image'].astype(str)\n    df.drop(columns=['image'], inplace=True)\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:43.738530Z","iopub.execute_input":"2024-11-01T07:34:43.738799Z","iopub.status.idle":"2024-11-01T07:34:43.743773Z","shell.execute_reply.started":"2024-11-01T07:34:43.738775Z","shell.execute_reply":"2024-11-01T07:34:43.742783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for item in ocr_train:\n    item[\"OCR\"] = \", \".join(item[\"OCR\"])\nfor item in ocr_test:\n    item[\"OCR\"] = \", \".join(item[\"OCR\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cap_test_df = json_to_df(cap_test)\n\ncap_train_df = json_to_df(cap_train)\n\nobj_train_df = json_to_df(obj_train)\n\nobj_test_df = json_to_df(obj_test)\n\nocr_train_df = json_to_df(ocr_train)\n\nocr_test_df = json_to_df(ocr_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:43.744864Z","iopub.execute_input":"2024-11-01T07:34:43.745183Z","iopub.status.idle":"2024-11-01T07:34:43.785512Z","shell.execute_reply.started":"2024-11-01T07:34:43.745146Z","shell.execute_reply":"2024-11-01T07:34:43.784611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"obj_train_df['object_recognition'] = obj_train_df['object_recognition'].apply(lambda x: \"Trong hình có \" + x + \". \" if len(x) > 0 else \"\")\nobj_test_df['object_recognition'] = obj_test_df['object_recognition'].apply(lambda x: \"Trong hình có \" + x + \". \" if len(x) > 0 else \"\")\nobj_test_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:43.786622Z","iopub.execute_input":"2024-11-01T07:34:43.786876Z","iopub.status.idle":"2024-11-01T07:34:43.804377Z","shell.execute_reply.started":"2024-11-01T07:34:43.786854Z","shell.execute_reply":"2024-11-01T07:34:43.803478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ocr_train_df['OCR'] = ocr_train_df['OCR'].apply(lambda row: \"Chữ trong hình là \" + row + \". \" if len(row) > 0 else \"\")\nocr_test_df['OCR'] = ocr_test_df['OCR'].apply(lambda row: \"Chữ trong hình là \" + row + \". \" if len(row) > 0 else \"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def enrich(df1, df2, add_field):\n    temp = df2.set_index('image_id')\n    \n    df1['caption'] = df1.apply(\n        lambda row: row['caption'] + ' ' + temp.loc[row['image_id'], add_field]\n        if row['image_id'] in temp.index else row['caption'], axis=1\n    )\n\n    return df1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:43.805471Z","iopub.execute_input":"2024-11-01T07:34:43.805788Z","iopub.status.idle":"2024-11-01T07:34:43.813887Z","shell.execute_reply.started":"2024-11-01T07:34:43.805761Z","shell.execute_reply":"2024-11-01T07:34:43.812998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['caption'] = train_df['caption'].apply(lambda x: x[:150] if len(x) > 150 else x)\ntest_df['caption'] = test_df['caption'].apply(lambda x: x[:150] if len(x) > 150 else x)\n\ntrain_df = enrich(train_df, ocr_train_df, 'OCR')\ntest_df = enrich(test_df, ocr_test_df, 'OCR')\n\n\ntrain_df = enrich(train_df, cap_train_df, 'caption')\ntest_df = enrich(test_df, cap_test_df, 'caption')\n\ntrain_df = enrich(train_df, obj_train_df, 'object_recognition')\ntest_df = enrich(test_df, obj_test_df, 'object_recognition')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:43.815045Z","iopub.execute_input":"2024-11-01T07:34:43.815331Z","iopub.status.idle":"2024-11-01T07:34:58.354394Z","shell.execute_reply.started":"2024-11-01T07:34:43.815307Z","shell.execute_reply":"2024-11-01T07:34:58.353442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:58.355632Z","iopub.execute_input":"2024-11-01T07:34:58.355930Z","iopub.status.idle":"2024-11-01T07:34:58.365712Z","shell.execute_reply.started":"2024-11-01T07:34:58.355905Z","shell.execute_reply":"2024-11-01T07:34:58.364695Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Text preprocessing","metadata":{}},{"cell_type":"code","source":"import re\ndef preprocess_text(text):\n    def remove_stopwords(text):\n        return text\n    def replace_emojis(text):\n        for emoji, description in emoji_dict.get('emoji', {}).items():\n            text = text.replace(emoji, description)  # Thay thế emoji bằng mô tả\n        return text\n\n    def replace_emoticons(text):\n        for emoticon, meaning in emoji_dict.get('biểu_tượng', {}).items():\n            emoticon_pattern = re.escape(emoticon) + r\"{1,}\"\n            text = re.sub(emoticon_pattern, meaning, text)\n        return text\n\n    def normalize_text(text):\n        text = text.lower()  # Chuyển thành chữ thường\n        text = re.sub(r'(?<=\\w)[\\/\\.\\-\\_,\\\\](?=\\w)', '', text)  # Loại bỏ dấu chấm hoặc gạch nối trong từ\n        return text\n\n    text = replace_emojis(text)       # Thay thế emoji\n    text = replace_emoticons(text)    # Thay thế biểu cảm\n    text = normalize_text(text)       # Chuẩn hóa văn bản\n    text = remove_stopwords(text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:58.366918Z","iopub.execute_input":"2024-11-01T07:34:58.367260Z","iopub.status.idle":"2024-11-01T07:34:58.376385Z","shell.execute_reply.started":"2024-11-01T07:34:58.367233Z","shell.execute_reply":"2024-11-01T07:34:58.375511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['caption'] = train_df['caption'].astype(str)\ntest_df['caption'] = test_df['caption'].astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:58.377469Z","iopub.execute_input":"2024-11-01T07:34:58.377752Z","iopub.status.idle":"2024-11-01T07:34:58.472145Z","shell.execute_reply.started":"2024-11-01T07:34:58.377727Z","shell.execute_reply":"2024-11-01T07:34:58.471348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['caption'] = train_df['caption'].apply(preprocess_text)\ntest_df['caption'] = test_df['caption'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:34:58.473200Z","iopub.execute_input":"2024-11-01T07:34:58.473463Z","iopub.status.idle":"2024-11-01T07:35:00.092483Z","shell.execute_reply.started":"2024-11-01T07:34:58.473440Z","shell.execute_reply":"2024-11-01T07:35:00.091486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['caption'].iloc[2003]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:35:00.100759Z","iopub.execute_input":"2024-11-01T07:35:00.101093Z","iopub.status.idle":"2024-11-01T07:35:00.107643Z","shell.execute_reply.started":"2024-11-01T07:35:00.101066Z","shell.execute_reply":"2024-11-01T07:35:00.106451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nX = train_df.drop(columns=['label'])  # Features\ny = train_df['label']  # Labels\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n\ntrain_df = pd.concat([X_train, y_train], axis=1)\nval_df = pd.concat([X_test, y_test], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:35:00.108754Z","iopub.execute_input":"2024-11-01T07:35:00.109043Z","iopub.status.idle":"2024-11-01T07:35:00.501140Z","shell.execute_reply.started":"2024-11-01T07:35:00.109018Z","shell.execute_reply":"2024-11-01T07:35:00.500088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df.shape, val_df.shape, test_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:35:00.502394Z","iopub.execute_input":"2024-11-01T07:35:00.502695Z","iopub.status.idle":"2024-11-01T07:35:00.507347Z","shell.execute_reply.started":"2024-11-01T07:35:00.502669Z","shell.execute_reply":"2024-11-01T07:35:00.506458Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Convert to Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, TrainingArguments, Seq2SeqTrainingArguments\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader\n\ndef preprocess_function(examples):\n    model_inputs = tokenizer(\n        examples[\"inputs\"], max_length=400, truncation=True, padding=True\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"labels\"], max_length=32, truncation=True, padding=True\n        )\n    model_inputs['labels'] = labels['input_ids']\n    model_inputs['input_ids'] = model_inputs['input_ids']\n    model_inputs[\"image_id\"] = examples[\"image_id\"]\n    \n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:35:00.508434Z","iopub.execute_input":"2024-11-01T07:35:00.508699Z","iopub.status.idle":"2024-11-01T07:35:01.009236Z","shell.execute_reply.started":"2024-11-01T07:35:00.508677Z","shell.execute_reply":"2024-11-01T07:35:01.008450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dict_obj = {}\n\ndict_obj['inputs'] = train_df['caption'].astype(str).tolist()  # Convert Series to list\ndict_obj['labels'] = train_df['label'].astype(str).tolist()    # Convert Series to list\ndict_obj['image_id'] = train_df['image_id'].astype(str).tolist()\ntrain_dataset = Dataset.from_dict(dict_obj)\ntokenized_train_datasets = train_dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=8)\n\ndict_obj = {}\ndict_obj['inputs'] = val_df['caption'].astype(str).tolist()  # Convert Series to list\ndict_obj['labels'] = val_df['label'].astype(str).tolist()    # Convert Series to list\ndict_obj['image_id'] = val_df['image_id'].astype(str).tolist()\nval_dataset = Dataset.from_dict(dict_obj)\ntokenized_val_datasets = val_dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:35:01.010399Z","iopub.execute_input":"2024-11-01T07:35:01.011380Z","iopub.status.idle":"2024-11-01T07:35:11.935276Z","shell.execute_reply.started":"2024-11-01T07:35:01.011349Z","shell.execute_reply":"2024-11-01T07:35:11.933688Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"import random\n\nimport numpy as np\n\nimport torch\n\ndef set_SEED():\n    SEED = 42\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.enabled = False\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:35:11.937380Z","iopub.execute_input":"2024-11-01T07:35:11.937770Z","iopub.status.idle":"2024-11-01T07:35:11.947893Z","shell.execute_reply.started":"2024-11-01T07:35:11.937730Z","shell.execute_reply":"2024-11-01T07:35:11.946860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom transformers.optimization import Adafactor, AdafactorSchedule\nfrom transformers import DataCollatorForSeq2Seq\nfrom utils import DataCollatorForSeq2Seq\n\nos.environ[\"WANDB_DISABLED\"] = \"True\"\nset_SEED()\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n\n#Adam\ntraining_args = Seq2SeqTrainingArguments(output_dir=\"./checkpoint\",\n                                      do_train=True,\n                                      do_eval=True,\n                                      num_train_epochs=1,\n                                      learning_rate=2.5e-5,\n                                      warmup_ratio=0.05,\n                                      weight_decay=0.01,\n                                      per_device_train_batch_size=8,\n                                      per_device_eval_batch_size=8,\n                                      logging_dir='./log',\n                                      group_by_length=True,\n                                      save_strategy=\"steps\",\n                                      evaluation_strategy=\"steps\",\n                                      save_total_limit=5,\n                                      eval_steps=100,\n                                      logging_steps = 100,\n                                      save_steps=100,\n                                      load_best_model_at_end= True,\n                                      fp16=True,\n                                      seed=42,\n                                      )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:35:11.952103Z","iopub.execute_input":"2024-11-01T07:35:11.952704Z","iopub.status.idle":"2024-11-01T07:35:11.968494Z","shell.execute_reply.started":"2024-11-01T07:35:11.952669Z","shell.execute_reply":"2024-11-01T07:35:11.967331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_train_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:35:11.969868Z","iopub.execute_input":"2024-11-01T07:35:11.970252Z","iopub.status.idle":"2024-11-01T07:35:11.976493Z","shell.execute_reply.started":"2024-11-01T07:35:11.970215Z","shell.execute_reply":"2024-11-01T07:35:11.975612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_val_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:35:11.977942Z","iopub.execute_input":"2024-11-01T07:35:11.978558Z","iopub.status.idle":"2024-11-01T07:35:11.989888Z","shell.execute_reply.started":"2024-11-01T07:35:11.978531Z","shell.execute_reply":"2024-11-01T07:35:11.988817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model = model,\n    args = training_args,\n    train_dataset=tokenized_train_datasets,\n    eval_dataset=tokenized_val_datasets,\n    data_collator=data_collator,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:35:11.991089Z","iopub.execute_input":"2024-11-01T07:35:11.991447Z","iopub.status.idle":"2024-11-01T07:50:49.915784Z","shell.execute_reply.started":"2024-11-01T07:35:11.991418Z","shell.execute_reply":"2024-11-01T07:50:49.914875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport json\n\n# save loss\nlog_history = {'log_history':trainer.state.log_history}\n\nwith open('logs.json', 'w', encoding='utf-8') as f:\n    json.dump(log_history, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:50:49.916960Z","iopub.execute_input":"2024-11-01T07:50:49.918000Z","iopub.status.idle":"2024-11-01T07:50:49.925019Z","shell.execute_reply.started":"2024-11-01T07:50:49.917965Z","shell.execute_reply":"2024-11-01T07:50:49.924181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib import pyplot\n\ntrain_loss = {i['step']:i['loss'] for i in log_history['log_history'] if 'loss' in i.keys()}\neval_loss = {i['step']:i['eval_loss'] for i in log_history['log_history'] if 'eval_loss' in i.keys()}\nplt.plot(list(train_loss.values()))\nplt.plot(list(eval_loss.values()))\n\nplt.yscale('log')\nplt.legend(['train','val'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T07:50:49.926244Z","iopub.execute_input":"2024-11-01T07:50:49.926799Z","iopub.status.idle":"2024-11-01T07:50:53.327573Z","shell.execute_reply.started":"2024-11-01T07:50:49.926764Z","shell.execute_reply":"2024-11-01T07:50:53.326601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load checkpoints and predict\n\n","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\n# model = T5ForConditionalGeneration.from_pretrained(\"/kaggle/working/checkpoint/checkpoint-1200\")\n# model.to('cuda')\n# model.add_imgw(img_w)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T08:09:50.270837Z","iopub.execute_input":"2024-11-01T08:09:50.271558Z","iopub.status.idle":"2024-11-01T08:09:53.305009Z","shell.execute_reply.started":"2024-11-01T08:09:50.271519Z","shell.execute_reply":"2024-11-01T08:09:53.303949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\ndict_obj = {}\ndict_obj['inputs'] = test_df['caption']\ndict_obj['labels'] =  test_df['label']\ndict_obj['image_id'] = test_df['image_id']\ntest_dataset = Dataset.from_dict(dict_obj)\ntokenized_test_datasets = test_dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T08:09:58.271003Z","iopub.execute_input":"2024-11-01T08:09:58.271887Z","iopub.status.idle":"2024-11-01T08:10:00.759805Z","shell.execute_reply.started":"2024-11-01T08:09:58.271854Z","shell.execute_reply":"2024-11-01T08:10:00.758711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T08:10:00.761746Z","iopub.execute_input":"2024-11-01T08:10:00.762057Z","iopub.status.idle":"2024-11-01T08:10:01.158759Z","shell.execute_reply.started":"2024-11-01T08:10:00.762020Z","shell.execute_reply":"2024-11-01T08:10:01.157928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_test_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T08:10:01.378646Z","iopub.execute_input":"2024-11-01T08:10:01.378959Z","iopub.status.idle":"2024-11-01T08:10:01.385673Z","shell.execute_reply.started":"2024-11-01T08:10:01.378926Z","shell.execute_reply":"2024-11-01T08:10:01.384650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch \nimport numpy as np\nfrom datasets import load_metric\nmetrics = load_metric('accuracy')\n\ntorch.cuda.empty_cache()\nmax_target_length = 10\ndataloader = torch.utils.data.DataLoader(tokenized_test_datasets, collate_fn=data_collator, batch_size=4) #replace tokenized_dev_datasets with tokenized_test_datasets\n\npredictions = []\nreferences = []\n\nfor i, batch in enumerate(tqdm(dataloader)):\n    # greedy search\n    outputs = model.generate(image_id = batch['image_id'],\n        input_ids=batch['input_ids'].to('cuda'),\n        max_length=max_target_length,\n        attention_mask=batch['attention_mask'].to('cuda'),\n        return_dict_in_generate=True, output_attentions=True,)\n\n    #beam search for now   \n    # outputs = model.generate(\n    #     image_id = np.repeat(batch['image_id'], 7),\n    #     input_ids=batch['input_ids'].to('cuda'),\n    #     max_length=max_target_length,\n    #     attention_mask=batch['attention_mask'].to('cuda'),\n    #     return_dict_in_generate=True, output_attentions=True,\n    #     num_beams=7,\n    #     no_repeat_ngram_size=2)\n    \n    with tokenizer.as_target_tokenizer():\n        outputs = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in outputs.sequences]\n        labels = np.where(batch['labels'] != -100,  batch['labels'], tokenizer.pad_token_id)\n        # actuals = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in labels]\n\n    predictions.extend(outputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T08:10:01.387485Z","iopub.execute_input":"2024-11-01T08:10:01.387751Z","iopub.status.idle":"2024-11-01T08:11:12.935430Z","shell.execute_reply.started":"2024-11-01T08:10:01.387716Z","shell.execute_reply":"2024-11-01T08:11:12.934384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T08:11:12.937517Z","iopub.execute_input":"2024-11-01T08:11:12.937977Z","iopub.status.idle":"2024-11-01T08:11:12.944774Z","shell.execute_reply.started":"2024-11-01T08:11:12.937940Z","shell.execute_reply":"2024-11-01T08:11:12.943758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def postprocessing(t):\n    t = t.lower()\n    t = t.replace('\\\\','')\n    return t\n\ntest_predicted = {k:postprocessing(i) for k,i in zip(dev_json.keys(),predictions)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T08:11:12.946411Z","iopub.execute_input":"2024-11-01T08:11:12.946759Z","iopub.status.idle":"2024-11-01T08:11:12.956901Z","shell.execute_reply.started":"2024-11-01T08:11:12.946726Z","shell.execute_reply":"2024-11-01T08:11:12.956095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_json = {\n    \"results\": test_predicted,\n    \"phase\": \"dev\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T08:11:12.959493Z","iopub.execute_input":"2024-11-01T08:11:12.959767Z","iopub.status.idle":"2024-11-01T08:11:12.967389Z","shell.execute_reply.started":"2024-11-01T08:11:12.959743Z","shell.execute_reply":"2024-11-01T08:11:12.966515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nwith open('results.json', 'w') as fp:\n    json.dump(result_json, fp,ensure_ascii=True,indent=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T08:11:13.021675Z","iopub.execute_input":"2024-11-01T08:11:13.021910Z","iopub.status.idle":"2024-11-01T08:11:13.030707Z","shell.execute_reply.started":"2024-11-01T08:11:13.021887Z","shell.execute_reply":"2024-11-01T08:11:13.029839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finetune for classification task","metadata":{}}]}